{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RNN (Recurrent Neural Network)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Theory**\n",
    "\n",
    "RNN stands for recurrent neural network, which is a type of artificial neural network that can process sequential data or time series data. Unlike feedforward neural networks, which assume that the inputs and outputs are independent of each other, RNNs can use their internal state (memory) to remember the previous inputs and outputs and use them to influence the current ones. This allows RNNs to exhibit temporal dynamic behavior and learn from the order and context of the data.\n",
    "\n",
    "RNNs are useful for tasks that involve sequential data, such as natural language processing, speech recognition, text generation, machine translation, sentiment analysis, image captioning, etc. For example, RNNs can take a sentence as an input and generate a translation or a summary as an output. RNNs can also generate new sentences or texts based on a given prompt or topic.\n",
    "\n",
    "RNNs have a cyclic structure, where the same network is applied to each element of the sequence. The network has three types of layers: an input layer, a hidden layer, and an output layer. The input layer takes one element of the sequence at a time and passes it to the hidden layer. The hidden layer performs some computation on the input and produces an output and a hidden state. The output is passed to the output layer, which generates the final output for that element. The hidden state is passed back to the hidden layer as an additional input for the next element of the sequence. This way, the hidden state acts as a memory that stores information from previous elements and influences the current output.\n",
    "\n",
    "\n",
    "The weights of the network are shared across each element of the sequence, meaning that they are updated by using information from all elements. This reduces the number of parameters and makes the network more efficient. However, RNNs also face some challenges, such as:\n",
    "\n",
    "- **Slow training speed**: computation is pretty slow since each timestep requires information from the previous timestep \n",
    "- **The vanishing gradient problem**: This occurs when the gradients (the values that are used to update the weights) become very small during backpropagation (the process of learning from errors). This makes it hard for the network to learn from long-term dependencies or distant elements in the sequence.\n",
    "- **The exploding gradient problem**: This occurs when the gradients become very large during backpropagation. This makes it hard for the network to converge or stabilize and can cause numerical instability or overflow errors.\n",
    "- **The difficulty of parallelization**: This occurs because RNNs have to process each element of the sequence sequentially, meaning that they cannot take advantage of parallel computing techniques that can speed up the computation.\n",
    "\n",
    "\n",
    "To overcome these challenges, various types of RNNs have been developed, such as:\n",
    "\n",
    "- **Long short-term memory (LSTM)**: This is a type of RNN that has a more complex hidden layer structure that consists of four components: an input gate, a forget gate, an output gate, and a cell state. These components allow LSTM to selectively remember or forget information from previous elements and control what information is passed to the next element. LSTM can handle long-term dependencies and avoid vanishing gradients better than simple RNNs.\n",
    "- **Gated recurrent unit (GRU)**: This is a type of RNN that has a simpler hidden layer structure than LSTM but still has two components: a reset gate and an update gate. These components allow GRU to selectively reset or update information from previous elements and control what information is passed to the next element. GRU can also handle long-term dependencies and avoid vanishing gradients better than simple RNNs but with fewer parameters than LSTM.\n",
    "- **Bidirectional RNN (BiRNN)**: This is a type of RNN that has two hidden layers that process the sequence in opposite directions: one from left to right and one from right to left. This allows BiRNN to capture both past and future information from each element and generate more accurate outputs. BiRNN can be combined with LSTM or GRU to form bidirectional LSTM (BiLSTM) or bidirectional GRU (BiGRU)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RNN Math Formulas**\n",
    "\n",
    "One of the basic math formulas of RNN is the formula for computing the output and the hidden state of a simple RNN at each time step. The formula is:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = W_{hy} h_t + b_y\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $h_t$ is the hidden state vector at time step $t$.\n",
    "- $y_t$ is the output vector at time step $t$.\n",
    "- $W_{hh}$ is the weight matrix for the recurrent connection between hidden states.\n",
    "- $W_{xh}$ is the weight matrix for the input-to-hidden connection.\n",
    "- $W_{hy}$ is the weight matrix for the hidden-to-output connection.\n",
    "- $b_h$ is the bias vector for the hidden layer.\n",
    "- $b_y$ is the bias vector for the output layer.\n",
    "- $x_t$ is the input vector at time step $t$.\n",
    "- $\\tanh$ is the hyperbolic tangent activation function.\n",
    "\n",
    "The formula shows how the hidden state and the output are computed by using a linear combination of the previous hidden state, the current input, and the bias terms, followed by an activation function. The hidden state acts as a memory that stores information from previous time steps and influences the current output.\n",
    "\n",
    "Another math formula of RNN is the formula for computing the loss function of a simple RNN for a given sequence. The formula is:\n",
    "\n",
    "$$\n",
    "L = - \\sum_{t=1}^T \\log p(y_t | x_1, ..., x_t)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $L$ is the loss function for a given sequence.\n",
    "- $T$ is the length of the sequence.\n",
    "- $y_t$ is the true output at time step $t$.\n",
    "- $x_1, ..., x_t$ are the inputs up to time step $t$.\n",
    "- $p(y_t | x_1, ..., x_t)$ is the probability of generating $y_t$ given the inputs up to time step $t$, which can be computed by applying a softmax function to the output vector $y_t$.\n",
    "\n",
    "The formula shows how the loss function is computed by summing up the negative log probabilities of generating each true output given the inputs up to that time step. The loss function measures how well the RNN predicts the outputs for a given sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
