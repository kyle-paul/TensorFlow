{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Regularization**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example**\n",
    "\n",
    "Let’s say we want to build a linear regression model to predict the house prices based on some features, such as the number of rooms, the size of the house, the location, etc. We have a training dataset of 100 houses with their features and prices, and we want to use gradient descent to find the optimal weights for our model.\n",
    "\n",
    "However, we notice that our model is overfitting the training data, meaning that it has a very low training error but a high test error when we evaluate it on new data. This means that our model is too complex and it learns the noise in the training data instead of the general patterns.\n",
    "\n",
    "To prevent overfitting, we can use regularization techniques such as L2 or L1 regularization. These techniques add a penalty term to the loss function that depends on the magnitude of the weights. The penalty term makes the model prefer smaller weights and reduces the complexity of the model.\n",
    "\n",
    "For example, if we use L2 regularization, our loss function becomes:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2n} \\sum{i=1}^n(y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^m w_j^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- L is the loss function with L2 regularization.\n",
    "- n is the number of training examples.\n",
    "- yi​ is the true price of the i-th house.\n",
    "- y^​i​ is the predicted price of the i-th house.\n",
    "- λ is the regularization parameter that controls the strength of the regularization.\n",
    "- m is the number of features.\n",
    "- wj​ is the weight of the j-th feature\n",
    "\n",
    "The first term in the loss function is the mean squared error (MSE) that measures how well our model fits the data. The second term is the L2 regularization term that penalizes large weights and makes them shrink towards zero. The regularization parameter λ determines how much we want to regularize our model. A larger λ means more regularization and less overfitting, but also more bias and less variance. A smaller λ means less regularization and more overfitting, but also less bias and more variance.\n",
    "\n",
    "By using regularization, we can improve the performance of our model on new data and avoid overfitting. We can also use other regularization techniques such as L1 or elastic net for different effects and trade-offs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Formula for some Regression Models**\n",
    "\n",
    "**Logistic Regression**\n",
    "\n",
    "Logistic regression is a type of binary classification model that predicts the probability of an example belonging to a positive class. The output function of logistic regression is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(Wx + b)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{y}$ is the predicted probability of the positive class.\n",
    "- $\\sigma$ is the sigmoid function that maps any real number to a value between 0 and 1.\n",
    "- $W$ is the weight matrix of the model.\n",
    "- $x$ is the input vector of features.\n",
    "- $b$ is the bias vector of the model.\n",
    "\n",
    "The loss function of logistic regression without regularization is:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $L$ is the loss function without regularization.\n",
    "- $n$ is the number of training examples.\n",
    "- $y_i$ is the true label of the i-th example (0 or 1).\n",
    "- $\\hat{y}_i$ is the predicted probability of the i-th example.\n",
    "\n",
    "The loss function of logistic regression with L2 regularization is:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] + \\lambda \\sum_{j=1}^m w_j^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $L$ is the loss function with L2 regularization.\n",
    "- $\\lambda$ is the regularization parameter that controls the strength of the regularization.\n",
    "- $m$ is the number of features.\n",
    "- $w_j$ is the weight of the j-th feature.\n",
    "\n",
    "\n",
    "**Softmax Regression**\n",
    "\n",
    "Softmax regression is a type of multiclass classification model that predicts the probability of an example belonging to one of K classes. The output function of softmax regression is:\n",
    "\n",
    "$$\n",
    "\\hat{y}_k = \\frac{\\exp(W_k x + b_k)}{\\sum_{j=1}^K \\exp(W_j x + b_j)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\hat{y}_k$ is the predicted probability of the k-th class.\n",
    "- $W_k$ is the weight vector of the k-th class.\n",
    "- $x$ is the input vector of features.\n",
    "- $b_k$ is the bias term of the k-th class.\n",
    "- $K$ is the number of classes.\n",
    "\n",
    "The loss function of softmax regression without regularization is:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log(\\hat{y}_{ik})\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $L$ is the loss function without regularization.\n",
    "- $n$ is the number of training examples.\n",
    "- $y_{ik}$ is the true label of the i-th example for the k-th class (0 or 1).\n",
    "- $\\hat{y}_{ik}$ is the predicted probability of the i-th example for the k-th class.\n",
    "\n",
    "The loss function of softmax regression with L2 regularization is:\n",
    "\n",
    "$$\n",
    "L = - \\frac{1}{n} \\sum_{i=1}^n \\sum_{k=1}^K y_{ik} \\log(\\hat{y}_{ik}) + \\lambda \\sum_{k=1}^K \\sum_{j=1}^m w_{kj}^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $L$ is the loss function with L2 regularization.\n",
    "- $\\lambda$ is the regularization parameter that controls the strength of the regularization.\n",
    "- $m$ is the number of features.\n",
    "- $w_{kj}$ is the weight of the j-th feature for the k-th class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
